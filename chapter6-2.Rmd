---
title: 'Chapter 6: Linear Model Selection and Regularization'
author: "Sang Truong"
date: "October 23, 2019"
output:
  html_document: default
  pdf_document: default
---
#Loading the data

```{r, warning=FALSE}
library(ISLR)
summary(Hitters)
head(Hitters)
```

There are some missing values here, so before we proceed we will remove them: 

```{r, warning=FALSE}
Hitters = na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
```

#Best subset regression
We will now use the package `leaps` to evaluate all the best-subset models.
```{r}
library(leaps)
regfit.full=regsubsets(Salary~.,data = Hitters)
summary(regfit.full)
```

It gives by default best-subsets up to size 8; lets increase that to 19, i.e. all the variables
```{r}
regfit.full=regsubsets(Salary~.,data = Hitters, nvmax = 19)
reg.summary = summary(regfit.full)
plot(reg.summary$cp, xlab="Number of Variables", ylab = "Cp")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], pch = 20, col = "red")
```
There is a plot method for the `regsubsets` object
```{r}
plot(regfit.full, scale="Cp",asp=2)
dev.new(width=5, height=5)
coef(regfit.full, 10)
```

#Forward stepwise selection
```{r}
regfit.fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
plot(regfit.fwd, scale = "Cp")
```

#Model selection using a validation set 
Let's make a trainin and validation set, so that we can choose a good subset model. We will do it using a slightly different approach from what was done in the book. 

```{r}
dim(Hitters)
set.seed(1)
train = sample(seq(263), 180, replace = FALSE)
train
regfit.fwd = regsubsets(Salary~., data = Hitters[train,], nvmax = 19, method = "forward")
```

Now we will make predictions on the observations not used for training. We know there are 19 models, so we set up some vectors to record the errors. we have to do a bit of work here, because there is no predict method for `regsubsets`.

```{r}
val.errors = rep(NA, 19)
?rep()
val.errors

x.test = model.matrix(Salary~., data=Hitters[-train,]) #Excluding the training data
for(i in 1:19){
  coefi = coef(regfit.fwd, id = i)
  pred = x.test[,names(coefi)]%*%coefi
  val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}

#x.test[,names(coefi)]
#coefi
#Hitters[-train,]
#?model.matrix()
#?coef

plot(sqrt(val.errors), ylab = "Root MSE", ylim = c(300,400), pch=19, type = "b")
points(sqrt(regfit.fwd$rss[-1]/180), col = "blue", pch = 19, type = "b")
legend("topright", legend = c("Training", "Validation"), col = c("blue", "black"), pch = 19)
```

As we expect, the training error goes down monotonically as the model gets bigger, but not so for the validation error. 

This was a little tedious - not having a predict method for `regsubsets`. So we will write one! 

```{r}
predict.regsubsets = function (object, newdata, id, ...){
  form = as.formula(object$call[[2]]) # Extracting formula from regsubsets$call
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  mat[,names(coefi)]%*%coefi
}
```

# Model selection by cross-validation

We will do a 10 folds cross-validation.

```{r}
set.seed(11)
folds = sample(rep(1:10, length = nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA, 10, 19)
cv.errors

for(k in 1:10){
  best.fit=regsubsets(Salary~., data=Hitters[folds!=k,], nvmax = 19, method = "forward")
  for(i in 1:19){
    pred = predict(best.fit, Hitters[folds == k,], id = i)
    cv.errors[k,i] = mean( (Hitters$Salary[folds == k]-pred)^2 )
  }
}

rmse.cv = sqrt(apply(cv.errors,2,mean))
plot(rmse.cv, pch = 19, type = "b")
```

# Ridge regression and the Lasso
We will use the package `glmnet`, which does not use the model formula language, so we will use set up an `x` and `y`. 

```{r}
library(glmnet)
x = model.matrix(Salary~.-1, data = Hitters)
y = Hitters$Salary
```

First we will fit a ridge-regression model. This is achieved by calling `glmnet` with `alpha = 0`. There is also a `cv.glmnet` function which will do the cross-validation for us. 

```{r}
fit.ridge = glmnet(x, y, alpha = 0)
plot(fit.ridge, xvar = "lambda", label = TRUE)
?cv.glmnet(x, y)
cv.ridge = cv.glmnet(x,y, alpha = 0)
plot(cv.ridge)
```

Now we run the lasso
```{r}
fit.lasso = glmnet(x, y, alpha = 1)
plot(fit.lasso, xvar = "lambda", label = TRUE)
#Cross-validation
cv.lasso = cv.glmnet(x,y, alpha = 1)
plot(cv.lasso)
coef(cv.lasso)
```

Suppose we want to use our earlier train/validation division to select the `lambda` for the lasso.
```{r}
lasso.tr = glmnet(x[train,], y[train])
lasso.tr
plot(lasso.tr, xvar = "lambda")
pred = predict(lasso.tr, x[-train,])
dim(pred) #83 rows (since testing test has 83 samples), 89 columns (since there is 89 values of lambda)
rmse = sqrt(apply((y[-train]-pred)^2,2,mean)) #Margin = 2: apply the function by column
plot(log(lasso.tr$lambda), rmse, type = "b", xlab = "LogLambda")
lam.best = lasso.tr$lambda[order(rmse)[1]] #Order function sort the data decendently 
lam.best
coef(lasso.tr,s=lam.best)
```




























